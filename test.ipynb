{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e30ea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[search] Fetching page 1 ...\n",
      "[search] Fetching page 2 ...\n",
      "[search] Fetching page 3 ...\n",
      "[search] Fetching page 4 ...\n",
      "[search] Reached max_items=200\n",
      "\n",
      "=== [1/200] rcgtn-Budget_Finance_Committee_-_August_10_2023 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [2/200] Committee_on_Finance_12-6-2017 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [3/200] lawin-Board_of_Public_Works_-_June_24_2021 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [4/200] 6980330-BOE-Approved-ShotSpotter-Flex-Service-Agreement ===\n",
      "[download] https://archive.org/download/6980330-BOE-Approved-ShotSpotter-Flex-Service-Agreement/6980330-BOE-Approved-ShotSpotter-Flex-Service-Agreement.pdf\n",
      "\n",
      "=== [5/200] cgmn-Cottage_Grove_City_Council_Meeting_10-19-22 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [6/200] BTWRLM121 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [7/200] manualzilla-id-5970347 ===\n",
      "[download] https://archive.org/download/manualzilla-id-5970347/5970347_djvu.txt\n",
      "\n",
      "=== [8/200] CIA-RDP83B00823R000100130009-4 ===\n",
      "[download] https://archive.org/download/CIA-RDP83B00823R000100130009-4/CIA-RDP83B00823R000100130009-4_djvu.txt\n",
      "\n",
      "=== [9/200] Committee_on_Finance_11-01-2017 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [10/200] 19PR0196RioHondoInstructionalServiceAgreement ===\n",
      "[download] https://archive.org/download/19PR0196RioHondoInstructionalServiceAgreement/19-PR0196 Rio Hondo Instructional Service Agreement_djvu.txt\n",
      "\n",
      "=== [11/200] in.gov.telangana.goir.2020-09-04.law-routine-411 ===\n",
      "[download] https://archive.org/download/in.gov.telangana.goir.2020-09-04.law-routine-411/law-routine-411_djvu.txt\n",
      "\n",
      "=== [12/200] lwrnks-08_22_23_City_Commission ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [13/200] DTIC_ADA208660 ===\n",
      "[download] https://archive.org/download/DTIC_ADA208660/DTIC_ADA208660_djvu.txt\n",
      "\n",
      "=== [14/200] 5694539-Sandy-Hill-Service-Agreement ===\n",
      "[download] https://archive.org/download/5694539-Sandy-Hill-Service-Agreement/5694539-Sandy-Hill-Service-Agreement_djvu.txt\n",
      "\n",
      "=== [15/200] coktx-Kyle_City_Council_Meeting_-_Dec._6_2021 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [16/200] cowomn-Woodbury_City_Council_Meeting_12-11-19 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [17/200] wbctvwi-School_Board_Meeting_-_July_24_2023 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [18/200] indioca-City_Council_Indio_Water_Authority_-_01_Mar_2023 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [19/200] kernca-Kern_County_Board_of_Supervisors_9_00_a.m._meeting_for_Tuesday_October_25_2016 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [20/200] lwrnks-04_12_22_City_Commission ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [21/200] lwrnks-07_12_22_City_Commission ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [22/200] cobmd-October_19 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [23/200] dgsvga-08-15-2019_City_Council_Legislative_Work_Session_Part_1 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [24/200] sfulga-City_of_South_Fulton_-_City_Council_Meeting_-_March_28_2023 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [25/200] coclwfl-City_of_Clearwater_Council_Work_Session_7_31_23 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [26/200] dgsvga-08-17-2020_-_City_Council_Regular_Meeting ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [27/200] natchila-Natchitoches_City_Council_Meeting_Monday_August_22_2022 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [28/200] coktx-Kyle_City_Council_Meeting_-_Sept_8th_2022 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [29/200] cobral-Brewton_City_Council_10_9_2017 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [30/200] cobral-Brewton_City_Council_12_28_2020 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [31/200] coewa-Everett_City_Council_Meeting_-_Dec._1_2021 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [32/200] gov.uscourts.nysd.465232 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [33/200] Vergennes_City_Council_-_September_11_2018 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [34/200] cowilv-City_Council_Meeting_8-5-19 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [35/200] City_of_Lathrop_City_Council_Meeting_-_April_1_2013 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [36/200] dgsvga-02-15-2021_-_City_Council_Regular_Meeting ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [37/200] dgsvga-04-06-2020_-_City_Council_Regular_Meeting ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [38/200] dgsvga-06-27-2019_City_Council_Legislative_Work_Session ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [39/200] dgsvga-09-12-2019_City_Council_Legislative_Work_Session ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [40/200] Formal_08_26_14_Session_-_Norfolk_City_Council ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [41/200] cocpga-CITY_OF_COLLEGE_PARK_MAYOR_AND_CITY_COUNCIL_REGULAR_SESSION_October_4_2021 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [42/200] englewco-City_Council_Regular_-_10_Jun_2019 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [43/200] coclwfl-City_of_Clearwater_Council_Meeting_5_4_23 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [44/200] coalbmi-Albion_Council_Study_Session_5_25_2021 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [45/200] cicctx-City_of_Corpus_Christi_Council_Approved_April_26_2022 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [46/200] cig_0152-Committee_on_Finance_2-9-2021 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [47/200] dgsvga-04-30-2020_-_City_Council_Legislative_Work_Session ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [48/200] city.-council.-09.01.2020 ===\n",
      "[download] https://archive.org/download/city.-council.-09.01.2020/City.Council.09.01.2020_djvu.txt\n",
      "\n",
      "=== [49/200] tbws91119 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [50/200] 111819tcbusiness_201911 ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [51/200] dgsvga-09-12-2019_City_Council_Legislative_Work_Session ===\n",
      "[warn] No text/PDF file found for this item.\n",
      "\n",
      "=== [52/200] cia-readingroom-document-cia-rdp60-00442r000100210060-5 ===\n",
      "[download] https://archive.org/download/cia-readingroom-document-cia-rdp60-00442r000100210060-5/cia-rdp60-00442r000100210060-5_djvu.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 234\u001b[39m\n\u001b[32m    231\u001b[39m QUERY = \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mservice agreement\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# set max_items=None to fetch all; or a number like 200 while testing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[43mscrape_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQUERY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_output_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marchive_service_agreements\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_items\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 203\u001b[39m, in \u001b[36mscrape_query\u001b[39m\u001b[34m(query, base_output_dir, max_items)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m text_file:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         text_path = \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m pdf_file:\n\u001b[32m    205\u001b[39m         pdf_path = download_file(identifier, pdf_file, item_dir)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mdownload_file\u001b[39m\u001b[34m(identifier, file_info, out_dir)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m local_path\n\u001b[32m    149\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[download] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m resp.raise_for_status()\n\u001b[32m    153\u001b[39m os.makedirs(out_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/urllib3/connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    752\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    755\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m     75\u001b[39m err = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import requests\n",
    "\n",
    "# ---------- CONSTANTS ----------\n",
    "\n",
    "ADV_SEARCH_URL = \"https://archive.org/advancedsearch.php\"\n",
    "METADATA_URL = \"https://archive.org/metadata/\"\n",
    "DOWNLOAD_BASE_URL = \"https://archive.org/download/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"ArchiveScraper/1.0 (contact: youremail@example.com)\"\n",
    "}\n",
    "\n",
    "# ---------- SEARCH HELPERS ----------\n",
    "\n",
    "def search_archive(\n",
    "    query: str,\n",
    "    rows: int = 50,\n",
    "    max_items: Optional[int] = None,\n",
    "    sleep: float = 1.0,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use Internet Archive Advanced Search API to get a list of items for a query.\n",
    "\n",
    "    :param query: Lucene-style query, e.g. '\"service agreement\"'\n",
    "    :param rows: results per page (max ~50 is safe)\n",
    "    :param max_items: stop after this many items (None => all)\n",
    "    :param sleep: seconds to sleep between requests (be polite)\n",
    "    :return: list of docs (each is a metadata dict with 'identifier', 'title', etc.)\n",
    "    \"\"\"\n",
    "    all_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"output\": \"json\",\n",
    "        \"rows\": rows,\n",
    "        \"page\": 1,\n",
    "        # choose the fields you want back\n",
    "        \"fl[]\": [\n",
    "            \"identifier\",\n",
    "            \"title\",\n",
    "            \"creator\",\n",
    "            \"year\",\n",
    "            \"mediatype\",\n",
    "            \"collection\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        print(f\"[search] Fetching page {params['page']} ...\")\n",
    "        resp = requests.get(ADV_SEARCH_URL, params=params, headers=HEADERS, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        response = data.get(\"response\", {})\n",
    "        docs = response.get(\"docs\", [])\n",
    "        num_found = response.get(\"numFound\", 0)\n",
    "\n",
    "        if not docs:\n",
    "            break\n",
    "\n",
    "        for d in docs:\n",
    "            all_docs.append(d)\n",
    "            if max_items is not None and len(all_docs) >= max_items:\n",
    "                print(f\"[search] Reached max_items={max_items}\")\n",
    "                return all_docs\n",
    "\n",
    "        # paging\n",
    "        total_pages = math.ceil(num_found / rows)\n",
    "        if params[\"page\"] >= total_pages:\n",
    "            break\n",
    "\n",
    "        params[\"page\"] += 1\n",
    "        time.sleep(sleep)  # be nice to archive.org\n",
    "\n",
    "    print(f\"[search] Collected {len(all_docs)} items (of {num_found} total).\")\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "# ---------- METADATA + DOWNLOAD HELPERS ----------\n",
    "\n",
    "def fetch_metadata(identifier: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get full metadata for a given item identifier.\"\"\"\n",
    "    url = METADATA_URL + identifier\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "def choose_text_file(files: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Try to find the best text-like file (OCR) from the metadata \"files\" list.\n",
    "    Preference order:\n",
    "        1. format \"DjvuTXT\" (common OCR text)\n",
    "        2. format \"Text\"\n",
    "        3. any file ending with .txt\n",
    "    \"\"\"\n",
    "    preferred_formats = [\"DjvuTXT\", \"Text\", \"TXT\"]\n",
    "\n",
    "    # 1) Try preferred formats\n",
    "    for fmt in preferred_formats:\n",
    "        for f in files:\n",
    "            if f.get(\"format\") == fmt:\n",
    "                return f\n",
    "\n",
    "    # 2) Fallback: any .txt file\n",
    "    for f in files:\n",
    "        name = f.get(\"name\", \"\").lower()\n",
    "        if name.endswith(\".txt\"):\n",
    "            return f\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def choose_pdf_file(files: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Fallback: choose a PDF file if no text file exists.\"\"\"\n",
    "    pdf_formats = [\"Text PDF\", \"PDF\"]\n",
    "    for fmt in pdf_formats:\n",
    "        for f in files:\n",
    "            if f.get(\"format\") == fmt:\n",
    "                return f\n",
    "\n",
    "    # Any .pdf as last resort\n",
    "    for f in files:\n",
    "        name = f.get(\"name\", \"\").lower()\n",
    "        if name.endswith(\".pdf\"):\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "\n",
    "def download_file(identifier: str, file_info: Dict[str, Any], out_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Download a single file from archive.org/download/{identifier}/{name}\n",
    "    Returns local file path.\n",
    "    \"\"\"\n",
    "    name = file_info[\"name\"]\n",
    "    url = f\"{DOWNLOAD_BASE_URL}{identifier}/{name}\"\n",
    "    local_path = os.path.join(out_dir, name)\n",
    "\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"[download] Already exists: {local_path}\")\n",
    "        return local_path\n",
    "\n",
    "    print(f\"[download] {url}\")\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "    return local_path\n",
    "\n",
    "\n",
    "# ---------- MAIN SCRAPE PIPELINE ----------\n",
    "\n",
    "def scrape_query(\n",
    "    query: str,\n",
    "    base_output_dir: str = \"archive_service_agreements\",\n",
    "    max_items: Optional[int] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level function:\n",
    "    - search for items\n",
    "    - for each item, fetch metadata\n",
    "    - download best text file (or PDF) if available\n",
    "    - write a JSONL manifest with metadata + local paths\n",
    "    \"\"\"\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "    manifest_path = os.path.join(base_output_dir, \"items.jsonl\")\n",
    "\n",
    "    docs = search_archive(query=query, rows=50, max_items=max_items)\n",
    "\n",
    "    with open(manifest_path, \"w\", encoding=\"utf-8\") as manifest_f:\n",
    "        for idx, doc in enumerate(docs, start=1):\n",
    "            identifier = doc[\"identifier\"]\n",
    "            print(f\"\\n=== [{idx}/{len(docs)}] {identifier} ===\")\n",
    "\n",
    "            try:\n",
    "                meta = fetch_metadata(identifier)\n",
    "            except Exception as e:\n",
    "                print(f\"[meta] Failed to fetch metadata for {identifier}: {e}\")\n",
    "                continue\n",
    "\n",
    "            files = meta.get(\"files\", []) or []\n",
    "\n",
    "            text_file = choose_text_file(files)\n",
    "            pdf_file = choose_pdf_file(files) if text_file is None else None\n",
    "\n",
    "            item_dir = os.path.join(base_output_dir, identifier)\n",
    "            os.makedirs(item_dir, exist_ok=True)\n",
    "\n",
    "            text_path = None\n",
    "            pdf_path = None\n",
    "\n",
    "            try:\n",
    "                if text_file:\n",
    "                    text_path = download_file(identifier, text_file, item_dir)\n",
    "                elif pdf_file:\n",
    "                    pdf_path = download_file(identifier, pdf_file, item_dir)\n",
    "                else:\n",
    "                    print(\"[warn] No text/PDF file found for this item.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[download] Error downloading files for {identifier}: {e}\")\n",
    "\n",
    "            # Build record for manifest\n",
    "            record = {\n",
    "                \"identifier\": identifier,\n",
    "                \"title\": doc.get(\"title\"),\n",
    "                \"creator\": doc.get(\"creator\"),\n",
    "                \"year\": doc.get(\"year\"),\n",
    "                \"mediatype\": doc.get(\"mediatype\"),\n",
    "                \"collection\": doc.get(\"collection\"),\n",
    "                \"details_url\": f\"https://archive.org/details/{identifier}\",\n",
    "                \"downloaded_text\": text_path,\n",
    "                \"downloaded_pdf\": pdf_path,\n",
    "            }\n",
    "\n",
    "            manifest_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n[done] Wrote manifest to: {manifest_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Same as: https://archive.org/search?query=%22service+agreement%22\n",
    "    QUERY = '\"service agreement\"'\n",
    "\n",
    "    # set max_items=None to fetch all; or a number like 200 while testing\n",
    "    scrape_query(query=QUERY, base_output_dir=\"archive_service_agreements\", max_items=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c6d230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking nda_brain\n",
      "   ‚Üí https://www.sec.gov/edgar/search-and-access ... OK (200)\n",
      "   ‚Üí https://www.sec.gov/edgar/search/#/q=nda&filter_forms=10-K ... OK (200)\n",
      "   ‚Üí https://github.com/ContractStandards/Contract-Clauses ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://onenda.org/ ... OK (200)\n",
      "   ‚Üí https://huggingface.co/datasets/atticus-project/cuad ... FAILED (401, HTTP 401)\n",
      "   ‚Üí https://github.com/jamesacampbell/contracts ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://github.com/alangrafu/legal-docs ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://www.dol.gov/agencies/oasam/site-closures/nda ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://archive.org/search?query=non-disclosure+agreement ... OK (200)\n",
      "\n",
      "üîç Checking msa_brain\n",
      "   ‚Üí https://www.sec.gov/edgar/search-and-access ... OK (200)\n",
      "   ‚Üí https://www.sec.gov/edgar/search/#/q=%22master%20service%20agreement%22 ... OK (200)\n",
      "   ‚Üí https://nvca.org/model-legal-documents/ ... OK (200)\n",
      "   ‚Üí https://www.techcontracts.com/resources/ ... OK (200)\n",
      "   ‚Üí https://huggingface.co/datasets/lex_glue ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://content.next.westlaw.com/practical-law ... FAILED (403, HTTP 403)\n",
      "   ‚Üí https://www.miamidade.gov/Apps/ContractSearch/ ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://www.data.gov/search?q=contract ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://sam.gov/content/opportunities ... OK (200)\n",
      "   ‚Üí https://archive.org/search?query=%22service+agreement%22 ... OK (200)\n",
      "\n",
      "üîç Checking investigation_brain\n",
      "   ‚Üí https://www.cs.cmu.edu/~enron/ ... OK (200)\n",
      "   ‚Üí https://www.sec.gov/enforcement ... OK (200)\n",
      "   ‚Üí https://www.justice.gov/news ... OK (200)\n",
      "   ‚Üí https://www.courtlistener.com/ ... OK (200)\n",
      "   ‚Üí https://www.oig.dol.gov/reports.htm ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://www.ftc.gov/legal-library/browse/cases-proceedings ... OK (200)\n",
      "   ‚Üí https://www.govinfo.gov/app/collection/chrg ... OK (200)\n",
      "   ‚Üí https://openjustice.doj.ca.gov/ ... FAILED (None, HTTPSConnectionPool(host='openjustice.doj.ca.gov', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x70dc4cce6f00>: Failed to resolve 'openjustice.doj.ca.gov' ([Errno -3] Temporary failure in name resolution)\")))\n",
      "   ‚Üí https://vault.fbi.gov/ ... FAILED (403, HTTP 403)\n",
      "   ‚Üí https://www.nist.gov/publications ... OK (200)\n",
      "\n",
      "üîç Checking case_intelligence_brain\n",
      "   ‚Üí https://www.courtlistener.com/api/bulk-data/ ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://www.recapthelaw.org/ ... FAILED (None, HTTPSConnectionPool(host='www.recapthelaw.org', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x70dc4cb10fe0>: Failed to resolve 'www.recapthelaw.org' ([Errno -5] No address associated with hostname)\")))\n",
      "   ‚Üí https://www.supremecourt.gov/opinions/opinions.aspx ... OK (200)\n",
      "   ‚Üí https://www.uscourts.gov/court-records ... OK (200)\n",
      "   ‚Üí https://www.govinfo.gov/app/collection/uscourts ... OK (200)\n",
      "   ‚Üí https://vault.fbi.gov/ ... FAILED (403, HTTP 403)\n",
      "   ‚Üí https://case.law/ ... OK (200)\n",
      "   ‚Üí https://opendata.cityofnewyork.us/ ... OK (200)\n",
      "\n",
      "üîç Checking new_york_module\n",
      "   ‚Üí https://public.leginfo.state.ny.us/laws/ ... FAILED (None, HTTPSConnectionPool(host='public.leginfo.state.ny.us', port=443): Max retries exceeded with url: /laws/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x70dc4cb389b0>, 'Connection to public.leginfo.state.ny.us timed out. (connect timeout=10)')))\n",
      "   ‚Üí https://www.nycourts.gov/courts/appeals/decisions/ ... FAILED (403, HTTP 403)\n",
      "   ‚Üí https://nycourts.gov/courts/ad1/Decisions.shtml ... FAILED (403, HTTP 403)\n",
      "   ‚Üí https://ag.ny.gov/press-releases ... OK (200)\n",
      "   ‚Üí https://opendata.cityofnewyork.us/ ... OK (200)\n",
      "   ‚Üí https://www.nysenate.gov/transparency/contracts ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://www.nyc.gov/site/law/publications/publications.page ... FAILED (404, HTTP 404)\n",
      "   ‚Üí https://www.nycourts.gov/courthelp/UGC/UCC.shtml ... FAILED (403, HTTP 403)\n",
      "   ‚Üí https://wwe1.osc.state.ny.us/transparency/contracts/contractsearch.cfm ... FAILED (404, HTTP 404)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ WORKING LINKS (FORMATTED LIKE INPUT)\n",
      "================================================================================\n",
      "{\n",
      "    \"nda_brain\": [\n",
      "        \"https://www.sec.gov/edgar/search-and-access\",\n",
      "        \"https://www.sec.gov/edgar/search/#/q=nda&filter_forms=10-K\",\n",
      "        \"https://onenda.org/\",\n",
      "        \"https://archive.org/search?query=non-disclosure+agreement\"\n",
      "    ],\n",
      "    \"msa_brain\": [\n",
      "        \"https://www.sec.gov/edgar/search-and-access\",\n",
      "        \"https://www.sec.gov/edgar/search/#/q=%22master%20service%20agreement%22\",\n",
      "        \"https://nvca.org/model-legal-documents/\",\n",
      "        \"https://www.techcontracts.com/resources/\",\n",
      "        \"https://sam.gov/content/opportunities\",\n",
      "        \"https://archive.org/search?query=%22service+agreement%22\"\n",
      "    ],\n",
      "    \"investigation_brain\": [\n",
      "        \"https://www.cs.cmu.edu/~enron/\",\n",
      "        \"https://www.sec.gov/enforcement\",\n",
      "        \"https://www.justice.gov/news\",\n",
      "        \"https://www.courtlistener.com/\",\n",
      "        \"https://www.ftc.gov/legal-library/browse/cases-proceedings\",\n",
      "        \"https://www.govinfo.gov/app/collection/chrg\",\n",
      "        \"https://www.nist.gov/publications\"\n",
      "    ],\n",
      "    \"case_intelligence_brain\": [\n",
      "        \"https://www.supremecourt.gov/opinions/opinions.aspx\",\n",
      "        \"https://www.uscourts.gov/court-records\",\n",
      "        \"https://www.govinfo.gov/app/collection/uscourts\",\n",
      "        \"https://case.law/\",\n",
      "        \"https://opendata.cityofnewyork.us/\"\n",
      "    ],\n",
      "    \"new_york_module\": [\n",
      "        \"https://ag.ny.gov/press-releases\",\n",
      "        \"https://opendata.cityofnewyork.us/\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "‚ùå FAILED LINKS (BRAIN-WISE)\n",
      "================================================================================\n",
      "{\n",
      "    \"nda_brain\": [\n",
      "        {\n",
      "            \"url\": \"https://github.com/ContractStandards/Contract-Clauses\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://huggingface.co/datasets/atticus-project/cuad\",\n",
      "            \"status\": 401,\n",
      "            \"error\": \"HTTP 401\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://github.com/jamesacampbell/contracts\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://github.com/alangrafu/legal-docs\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://www.dol.gov/agencies/oasam/site-closures/nda\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        }\n",
      "    ],\n",
      "    \"msa_brain\": [\n",
      "        {\n",
      "            \"url\": \"https://huggingface.co/datasets/lex_glue\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://content.next.westlaw.com/practical-law\",\n",
      "            \"status\": 403,\n",
      "            \"error\": \"HTTP 403\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://www.miamidade.gov/Apps/ContractSearch/\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://www.data.gov/search?q=contract\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        }\n",
      "    ],\n",
      "    \"investigation_brain\": [\n",
      "        {\n",
      "            \"url\": \"https://www.oig.dol.gov/reports.htm\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://openjustice.doj.ca.gov/\",\n",
      "            \"status\": null,\n",
      "            \"error\": \"HTTPSConnectionPool(host='openjustice.doj.ca.gov', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\\\"<urllib3.connection.HTTPSConnection object at 0x70dc4cce6f00>: Failed to resolve 'openjustice.doj.ca.gov' ([Errno -3] Temporary failure in name resolution)\\\"))\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://vault.fbi.gov/\",\n",
      "            \"status\": 403,\n",
      "            \"error\": \"HTTP 403\"\n",
      "        }\n",
      "    ],\n",
      "    \"case_intelligence_brain\": [\n",
      "        {\n",
      "            \"url\": \"https://www.courtlistener.com/api/bulk-data/\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://www.recapthelaw.org/\",\n",
      "            \"status\": null,\n",
      "            \"error\": \"HTTPSConnectionPool(host='www.recapthelaw.org', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\\\"<urllib3.connection.HTTPSConnection object at 0x70dc4cb10fe0>: Failed to resolve 'www.recapthelaw.org' ([Errno -5] No address associated with hostname)\\\"))\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://vault.fbi.gov/\",\n",
      "            \"status\": 403,\n",
      "            \"error\": \"HTTP 403\"\n",
      "        }\n",
      "    ],\n",
      "    \"new_york_module\": [\n",
      "        {\n",
      "            \"url\": \"https://public.leginfo.state.ny.us/laws/\",\n",
      "            \"status\": null,\n",
      "            \"error\": \"HTTPSConnectionPool(host='public.leginfo.state.ny.us', port=443): Max retries exceeded with url: /laws/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x70dc4cb389b0>, 'Connection to public.leginfo.state.ny.us timed out. (connect timeout=10)'))\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://www.nycourts.gov/courts/appeals/decisions/\",\n",
      "            \"status\": 403,\n",
      "            \"error\": \"HTTP 403\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://nycourts.gov/courts/ad1/Decisions.shtml\",\n",
      "            \"status\": 403,\n",
      "            \"error\": \"HTTP 403\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://www.nysenate.gov/transparency/contracts\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://www.nyc.gov/site/law/publications/publications.page\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://www.nycourts.gov/courthelp/UGC/UCC.shtml\",\n",
      "            \"status\": 403,\n",
      "            \"error\": \"HTTP 403\"\n",
      "        },\n",
      "        {\n",
      "            \"url\": \"https://wwe1.osc.state.ny.us/transparency/contracts/contractsearch.cfm\",\n",
      "            \"status\": 404,\n",
      "            \"error\": \"HTTP 404\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# =======================\n",
    "# PASTE YOUR BRAINS HERE\n",
    "# =======================\n",
    "BRAINS = {\n",
    "    \"nda_brain\": [\n",
    "        \"https://www.sec.gov/edgar/search-and-access\",\n",
    "        \"https://www.sec.gov/edgar/search/#/q=nda&filter_forms=10-K\",\n",
    "        \"https://github.com/ContractStandards/Contract-Clauses\",\n",
    "        \"https://onenda.org/\",\n",
    "        \"https://huggingface.co/datasets/atticus-project/cuad\",\n",
    "        \"https://github.com/jamesacampbell/contracts\",\n",
    "        \"https://github.com/alangrafu/legal-docs\",\n",
    "        \"https://www.dol.gov/agencies/oasam/site-closures/nda\",\n",
    "        \"https://archive.org/search?query=non-disclosure+agreement\",\n",
    "    ],\n",
    "    \"msa_brain\": [\n",
    "        \"https://www.sec.gov/edgar/search-and-access\",\n",
    "        \"https://www.sec.gov/edgar/search/#/q=%22master%20service%20agreement%22\",\n",
    "        \"https://nvca.org/model-legal-documents/\",\n",
    "        \"https://www.techcontracts.com/resources/\",\n",
    "        \"https://huggingface.co/datasets/lex_glue\",\n",
    "        \"https://content.next.westlaw.com/practical-law\",\n",
    "        \"https://www.miamidade.gov/Apps/ContractSearch/\",\n",
    "        \"https://www.data.gov/search?q=contract\",\n",
    "        \"https://sam.gov/content/opportunities\",\n",
    "        \"https://archive.org/search?query=%22service+agreement%22\",\n",
    "    ],\n",
    "    \"investigation_brain\": [\n",
    "        \"https://www.cs.cmu.edu/~enron/\",\n",
    "        \"https://www.sec.gov/enforcement\",\n",
    "        \"https://www.justice.gov/news\",\n",
    "        \"https://www.courtlistener.com/\",\n",
    "        \"https://www.oig.dol.gov/reports.htm\",\n",
    "        \"https://www.ftc.gov/legal-library/browse/cases-proceedings\",\n",
    "        \"https://www.govinfo.gov/app/collection/chrg\",\n",
    "        \"https://openjustice.doj.ca.gov/\",\n",
    "        \"https://vault.fbi.gov/\",\n",
    "        \"https://www.nist.gov/publications\",\n",
    "    ],\n",
    "    \"case_intelligence_brain\": [\n",
    "        \"https://www.courtlistener.com/api/bulk-data/\",\n",
    "        \"https://www.recapthelaw.org/\",\n",
    "        \"https://www.supremecourt.gov/opinions/opinions.aspx\",\n",
    "        \"https://www.uscourts.gov/court-records\",\n",
    "        \"https://www.govinfo.gov/app/collection/uscourts\",\n",
    "        \"https://vault.fbi.gov/\",\n",
    "        \"https://case.law/\",\n",
    "        \"https://opendata.cityofnewyork.us/\",\n",
    "    ],\n",
    "    \"new_york_module\": [\n",
    "        \"https://public.leginfo.state.ny.us/laws/\",\n",
    "        \"https://www.nycourts.gov/courts/appeals/decisions/\",\n",
    "        \"https://nycourts.gov/courts/ad1/Decisions.shtml\",\n",
    "        \"https://ag.ny.gov/press-releases\",\n",
    "        \"https://opendata.cityofnewyork.us/\",\n",
    "        \"https://www.nysenate.gov/transparency/contracts\",\n",
    "        \"https://www.nyc.gov/site/law/publications/publications.page\",\n",
    "        \"https://www.nycourts.gov/courthelp/UGC/UCC.shtml\",\n",
    "        \"https://wwe1.osc.state.ny.us/transparency/contracts/contractsearch.cfm\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# =======================================\n",
    "# CONFIG\n",
    "# =======================================\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; LegalRAGLinkChecker/1.0)\"\n",
    "}\n",
    "TIMEOUT = 10\n",
    "\n",
    "\n",
    "def check_url(url: str):\n",
    "    \"\"\"Checks URL, returns (ok, status, error_msg).\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT, allow_redirects=True)\n",
    "        status = resp.status_code\n",
    "        if status < 400:\n",
    "            return True, status, None\n",
    "        return False, status, f\"HTTP {status}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return False, None, str(e)\n",
    "\n",
    "\n",
    "def process_brains(brains: Dict[str, List[str]]):\n",
    "    working = {}\n",
    "    failed = {}\n",
    "\n",
    "    for brain, urls in brains.items():\n",
    "        print(f\"\\nüîç Checking {brain}\")\n",
    "        working[brain] = []\n",
    "        failed[brain] = []\n",
    "\n",
    "        for url in urls:\n",
    "            print(f\"   ‚Üí {url} ... \", end=\"\")\n",
    "            ok, status, err = check_url(url)\n",
    "\n",
    "            if ok:\n",
    "                print(f\"OK ({status})\")\n",
    "                working[brain].append(url)\n",
    "            else:\n",
    "                print(f\"FAILED ({status}, {err})\")\n",
    "                failed[brain].append({\n",
    "                    \"url\": url,\n",
    "                    \"status\": status,\n",
    "                    \"error\": err\n",
    "                })\n",
    "\n",
    "        if not failed[brain]:\n",
    "            del failed[brain]\n",
    "\n",
    "    return working, failed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    working_links, failed_links = process_brains(BRAINS)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ WORKING LINKS (FORMATTED LIKE INPUT)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(json.dumps(working_links, indent=4))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚ùå FAILED LINKS (BRAIN-WISE)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(json.dumps(failed_links, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d16f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched page 1, total: 100/684\n",
      "Fetched page 2, total: 200/684\n",
      "Fetched page 3, total: 300/684\n",
      "Fetched page 4, total: 400/684\n",
      "Fetched page 5, total: 500/684\n",
      "Fetched page 6, total: 600/684\n",
      "Fetched page 7, total: 684/684\n",
      "\n",
      "=== City.Council.04.17.2018 ===\n",
      "\n",
      "=== City_of_Lathrop_City_Council_Meeting_November_3_2014 ===\n",
      "\n",
      "=== cobral-Brewton_City_Council_9_9_2019 ===\n",
      "Downloading: https://archive.org/download/cobral-Brewton_City_Council_9_9_2019/Brewton_City_Council_9_9_2019.es.vtt\n",
      "\n",
      "=== coltcav-City_Council_Regular_Meeting_-_July_13th_2020 ===\n",
      "\n",
      "=== cowilv-City_Council_Meeting_6-3-19 ===\n",
      "\n",
      "=== toflaz-Town_Council_Meeting_June_18_2018 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 176\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDONE.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice agreement\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 152\u001b[39m, in \u001b[36mscrape\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m    148\u001b[39m os.makedirs(folder, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    150\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m meta = \u001b[43mfetch_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m files = meta.get(\u001b[33m\"\u001b[39m\u001b[33mfiles\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m    155\u001b[39m txt_file, pdf_file = find_text_or_pdf(files)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mfetch_metadata\u001b[39m\u001b[34m(identifier)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_metadata\u001b[39m(identifier: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMETADATA_URL\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEADERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     r.raise_for_status()\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Awais-project/DATACCOLLECTION/myenv/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pypdf import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "ADV_SEARCH_URL = \"https://archive.org/advancedsearch.php\"\n",
    "METADATA_URL = \"https://archive.org/metadata/\"\n",
    "DOWNLOAD_BASE_URL = \"https://archive.org/download/\"\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"LegalRAGBot/1.0\"}\n",
    "\n",
    "def safe(s: str) -> str:\n",
    "    return \"\".join(c for c in s if c.isalnum() or c in \" ._-\").strip()\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# SEARCH FUNCTION\n",
    "# ----------------------------------------------\n",
    "def search_archive(query: str, rows=100, sleep=1.0):\n",
    "    all_docs = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"output\": \"json\",\n",
    "            \"rows\": rows,\n",
    "            \"page\": page,\n",
    "            \"fl[]\": [\"identifier\", \"title\", \"creator\", \"year\"]\n",
    "        }\n",
    "\n",
    "        r = requests.get(ADV_SEARCH_URL, params=params, headers=HEADERS, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()[\"response\"]\n",
    "\n",
    "        docs = data.get(\"docs\", [])\n",
    "        total = data.get(\"numFound\", 0)\n",
    "\n",
    "        if not docs:\n",
    "            break\n",
    "\n",
    "        all_docs.extend(docs)\n",
    "        print(f\"Fetched page {page}, total: {len(all_docs)}/{total}\")\n",
    "\n",
    "        total_pages = math.ceil(total / rows)\n",
    "        if page >= total_pages:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(sleep)\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# DOWNLOAD FUNCTIONS\n",
    "# ----------------------------------------------\n",
    "def fetch_metadata(identifier: str):\n",
    "    r = requests.get(METADATA_URL + identifier, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def find_text_or_pdf(files):\n",
    "    txt = None\n",
    "    pdf = None\n",
    "\n",
    "    for f in files:\n",
    "        fmt = f.get(\"format\", \"\").lower()\n",
    "        name = f.get(\"name\", \"\").lower()\n",
    "\n",
    "        if \"djvutxt\" in fmt or \"text\" in fmt or name.endswith(\".txt\"):\n",
    "            txt = f\n",
    "\n",
    "        if \"pdf\" in fmt or name.endswith(\".pdf\"):\n",
    "            pdf = f\n",
    "\n",
    "    return txt, pdf\n",
    "\n",
    "\n",
    "def download(identifier, fileinfo, outdir):\n",
    "    name = fileinfo[\"name\"]\n",
    "    url = f\"{DOWNLOAD_BASE_URL}{identifier}/{name}\"\n",
    "    path = os.path.join(outdir, name)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading: {url}\")\n",
    "        r = requests.get(url, headers=HEADERS, timeout=60)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# TEXT EXTRACTION\n",
    "# ----------------------------------------------\n",
    "def extract_pdf_text(pdf_path):\n",
    "    text = []\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        for page in reader.pages:\n",
    "            content = page.extract_text() or \"\"\n",
    "            text.append(content)\n",
    "    except:\n",
    "        return \"\"\n",
    "    return \"\\n\\n\".join(text)\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# CREATE DOCX FOR EACH DOCUMENT\n",
    "# ----------------------------------------------\n",
    "def save_docx(title, text, outdir):\n",
    "    doc_path = os.path.join(outdir, safe(title) + \".docx\")\n",
    "    doc = Document()\n",
    "\n",
    "    # Bold title\n",
    "    h = doc.add_heading(level=1)\n",
    "    run = h.add_run(title)\n",
    "    run.bold = True\n",
    "\n",
    "    doc.add_paragraph(text or \"No text found.\")\n",
    "\n",
    "    doc.save(doc_path)\n",
    "    return doc_path\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# MAIN PIPELINE\n",
    "# ----------------------------------------------\n",
    "def scrape(query):\n",
    "    output_root = \"archive_scraped\"\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "    docs = search_archive(query)\n",
    "\n",
    "    for d in docs:\n",
    "        identifier = d[\"identifier\"]\n",
    "        title = d.get(\"title\") or identifier\n",
    "        folder_name = safe(title)\n",
    "        folder = os.path.join(output_root, folder_name)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        print(f\"\\n=== {identifier} ===\")\n",
    "\n",
    "        meta = fetch_metadata(identifier)\n",
    "        files = meta.get(\"files\", [])\n",
    "\n",
    "        txt_file, pdf_file = find_text_or_pdf(files)\n",
    "\n",
    "        text_content = \"\"\n",
    "\n",
    "        if txt_file:\n",
    "            txt_path = download(identifier, txt_file, folder)\n",
    "            text_content = open(txt_path, \"r\", errors=\"ignore\").read()\n",
    "\n",
    "        elif pdf_file:\n",
    "            pdf_path = download(identifier, pdf_file, folder)\n",
    "            text_content = extract_pdf_text(pdf_path)\n",
    "\n",
    "        else:\n",
    "            text_content = \"No OCR text or PDF found.\"\n",
    "\n",
    "        save_docx(title, text_content, folder)\n",
    "\n",
    "    print(\"\\nDONE.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape('\"service agreement\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510b41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
